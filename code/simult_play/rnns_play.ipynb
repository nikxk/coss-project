{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actions:\n",
    "    def __init__(self, actions):\n",
    "        self.all_actions = actions\n",
    "        self.history = [np.random.randint(len(actions))]\n",
    "    \n",
    "    def next(self, action):\n",
    "        self.history.append(action)\n",
    "\n",
    "    def last(self):\n",
    "        return self.history[-1]\n",
    "\n",
    "class Costs:\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "        self.accumulated = 0\n",
    "\n",
    "    def receive(self, cost):\n",
    "        self.accumulated += cost\n",
    "        self.history.append(cost)\n",
    "\n",
    "    def last(self):\n",
    "        return self.history[-1]\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, input_size=2, hidden_size=4, num_layers=2):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.rnn.parameters(), lr=0.01)\n",
    "\n",
    "    def train(self, cost):\n",
    "        self.optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "brain = Brain()\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = Actions(actions)\n",
    "        self.costs = Costs()\n",
    "\n",
    "        self.brain = brain\n",
    "        self.h = torch.randn(brain.num_layers, 1, brain.hidden_size)\n",
    "\n",
    "    def take_action(self, state):\n",
    "        self.out, self.h = self.brain.rnn(torch.as_tensor(state, dtype=torch.float32).view(1, 1, brain.input_size), self.h)\n",
    "        action = self.out.argmax().item()\n",
    "        self.actions.next(action)\n",
    "        return action\n",
    "\n",
    "    def receive_cost(self, cost):\n",
    "        self.costs.receive(cost)\n",
    "\n",
    "    def take_action_train(self, state):\n",
    "        self.out, self.h = self.brain.rnn(torch.as_tensor(state, dtype=torch.float32).view(1, 1, brain.input_size), self.h)\n",
    "        self.train_action = torch.softmax(self.out, dim=2)\n",
    "        return self.train_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CongestionGame:\n",
    "    def __init__(self, n_players=60, cost_setup=(15, 2, 0.2, 22.96)):\n",
    "        self.cost_f, self.cost_a, self.cost_a_mult, self.cost_best = cost_setup\n",
    "        self.actions = ['fa', 'fbf', 'af', 'aba']\n",
    "\n",
    "        self.init_players(n_players)\n",
    "\n",
    "\n",
    "    def init_players(self, n_players):\n",
    "        self.players = [Player(self.actions) for _ in range(n_players)]\n",
    "        first_actions = [player.actions.last() for player in self.players]\n",
    "\n",
    "        self.n_a1 = first_actions.count(self.actions.index('af')) + first_actions.count(self.actions.index('aba'))\n",
    "        self.n_a2 = first_actions.count(self.actions.index('fa')) + first_actions.count(self.actions.index('aba')) \n",
    "\n",
    "    def train_players(self, n_games=1):\n",
    "        for _ in range(n_games):\n",
    "            actions_sum = torch.zeros(1, 1, len(self.actions))\n",
    "            for player in self.players:\n",
    "                actions_sum += player.take_action_train((self.n_a1, self.n_a2))\n",
    "            actions_sum = actions_sum.squeeze()\n",
    "            self.n_a1 = actions_sum[2] + actions_sum[3]\n",
    "            self.n_a2 = actions_sum[0] + actions_sum[3]\n",
    "\n",
    "            self.total_cost = torch.tensor(0.0, requires_grad=True)\n",
    "            self.total_cost = \\\n",
    "                actions_sum[0] * (self.cost_f + self.cost_a + self.cost_a_mult * self.n_a2) +\\\n",
    "                actions_sum[1] * (2 * self.cost_f ) +\\\n",
    "                actions_sum[2] * (self.cost_f + self.cost_a + self.cost_a_mult * self.n_a1) +\\\n",
    "                actions_sum[3] * (2 * self.cost_a + self.cost_a_mult * (self.n_a1 + self.n_a2))\n",
    "\n",
    "            brain.train(self.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = CongestionGame()\n",
    "game.players[0].actions.last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.9331, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.n_a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1382.1665, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m game\u001b[39m.\u001b[39;49mtrain_players(\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn [66], line 32\u001b[0m, in \u001b[0;36mCongestionGame.train_players\u001b[1;34m(self, n_games)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_cost \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.0\u001b[39m, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_cost \u001b[39m=\u001b[39m \\\n\u001b[0;32m     27\u001b[0m     actions_sum[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_f \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_a \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_a_mult \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_a2) \u001b[39m+\u001b[39m\\\n\u001b[0;32m     28\u001b[0m     actions_sum[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_f ) \u001b[39m+\u001b[39m\\\n\u001b[0;32m     29\u001b[0m     actions_sum[\u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_f \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_a \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_a_mult \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_a1) \u001b[39m+\u001b[39m\\\n\u001b[0;32m     30\u001b[0m     actions_sum[\u001b[39m3\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_a \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcost_a_mult \u001b[39m*\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_a1 \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_a2))\n\u001b[1;32m---> 32\u001b[0m brain\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtotal_cost)\n",
      "Cell \u001b[1;32mIn [21], line 36\u001b[0m, in \u001b[0;36mBrain.train\u001b[1;34m(self, cost)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, cost):\n\u001b[0;32m     35\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 36\u001b[0m     cost\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     37\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nsk\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\nsk\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "game.train_players(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, input_size=2, hidden_size=4, num_layers=2):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.rnn.parameters(), lr=0.01)\n",
    "\n",
    "    def train(self, action, cost):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.loss_fn(action, cost).backward()\n",
    "\n",
    "brain = Brain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0837, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, brain.input_size)\n",
    "h = torch.randn(brain.num_layers, 1, brain.hidden_size)\n",
    "out, h = brain.rnn(x, h)\n",
    "action = torch.softmax(out, dim=2)\n",
    "action.squeeze()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nsk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d12a422309641c6d1478d94e0cbea5c43052e292756ae0ab0ed5769db3671a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
